import requests
from bs4 import BeautifulSoup
import json
import csv
import time
import datetime
import logging
import os
import re
import random
import argparse
from typing import Dict, List, Any, Optional
from urllib.parse import urljoin, urlparse
import ssl

# 配置常數
BASE_URL = "https://www.atmovies.com.tw/showtime/"
OUTPUT_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), "data")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 爬蟲配置
MAX_RETRIES = 5  # 增加重試次數
TIMEOUT = 60  # 增加請求超時時間（秒）
CONCURRENT_REQUESTS = 3  # 減少並發請求數
MAX_THEATERS_TO_CHECK = 5  # 減少要檢查的電影院數量
DAYS_TO_CHECK = 3  # 要檢查的天數（今天、明天、後天）

# 代理設置（如果需要，取消註釋並設置您的代理）
PROXIES = {
    # 'http': 'http://your-proxy-address:port',
    # 'https': 'http://your-proxy-address:port',
}

# 基礎請求頭
BASE_HEADERS = {
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Referer': 'https://www.google.com/',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'same-origin',
    'Sec-Fetch-User': '?1',
    'Cache-Control': 'max-age=0',
    'TE': 'trailers',
    'DNT': '1',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"macOS"',
    'sec-gpc': '1'
}

try:
    from .user_agents import USER_AGENTS
except ImportError:
    # 如果無法從當前目錄導入，嘗試直接定義 USER_AGENTS
    USER_AGENTS = [
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0"
    ]

def get_random_user_agent() -> str:
    """從 USER_AGENTS 列表中隨機選擇一個 User-Agent"""
    return random.choice(USER_AGENTS)

def get_headers() -> Dict[str, str]:
    """獲取帶有隨機 User-Agent 的請求頭"""
    headers = BASE_HEADERS.copy()
    headers['User-Agent'] = get_random_user_agent()
    headers['sec-gpc'] = '1'
    return headers

# 爬蟲常數
MAX_RETRIES = 3  # 最大重試次數
TIMEOUT = 30  # 請求超時時間增加到30秒
MAX_CONCURRENT_REQUESTS = 3  # 降低並發請求數，避免觸發反爬蟲
MAX_THEATERS_TO_CHECK_DEFAULT = 10  # 預設要檢查的電影院數量

# 設定日誌
log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'logs')
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, f'atmovies_update_checker_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.log')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# 確保輸出目錄存在 (雖然此腳本不直接輸出文件，但保留以防萬一或繼承自原腳本的邏輯)
os.makedirs(OUTPUT_DIR, exist_ok=True)
# print(f"輸出目錄設置為: {OUTPUT_DIR}") # Not critical for this script

class ATMoviesScraper:
    def __init__(self):
        self.data = [] # Still collect data, but won't be saved to file by default
        self.proxy = None
    
    def fetch_page(self, url: str, max_retries: int = 3) -> Optional[BeautifulSoup]:
        """
        獲取並解析網頁內容，自動重試失敗的請求
        
        Args:
            url: 要獲取的網址
            max_retries: 最大重試次數
            
        Returns:
            解析後的 BeautifulSoup 對象，如果獲取或解析失敗則返回 None
        """
        headers = get_headers()
        retries = 0
        last_exception = None
        
        while retries <= max_retries:
            try:
                # 設置請求參數
                request_kwargs = {
                    'headers': headers,
                    'timeout': 30,  # 30秒超時
                    'verify': False  # 跳過SSL驗證
                }
                
                # 如果設置了代理，則添加代理
                if self.proxy:
                    request_kwargs['proxies'] = {
                        'http': self.proxy,
                        'https': self.proxy
                    }
                
                logger.info(f"正在抓取: {url} (嘗試 {retries + 1}/{max_retries + 1})")
                
                response = requests.get(url, **request_kwargs)
                
                # 檢查 HTTP 狀態碼
                if response.status_code != 200:
                    if response.status_code == 404:
                        logger.warning(f"頁面不存在 (404): {url}")
                        return None  # 404 直接返回 None
                    raise requests.RequestException(f"HTTP 錯誤: {response.status_code}")
                
                # 讀取響應內容
                content = response.text
                
                # 檢查內容是否有效
                if not content or len(content) < 100:  # 假設有效內容至少100字節
                    raise ValueError(f"無效的響應內容，長度: {len(content)}")
                
                # 檢查是否為錯誤頁面
                soup = BeautifulSoup(content, 'html.parser')
                if self._is_error_page(soup):
                    raise ValueError("檢測到錯誤頁面")
                
                return soup
                
            except requests.exceptions.Timeout:
                logger.warning(f"請求超時: {url}")
                last_exception = "請求超時"
                time.sleep(5)  # 等待5秒後重試
                retries += 1
                if retries > max_retries:
                    logger.error(f"所有重試次數已用盡，無法獲取頁面: {url}")
                    return None
                    
            except requests.exceptions.RequestException as e:
                error_msg = str(e) if str(e) else f"HTTP {getattr(e.response, 'status_code', 'Unknown') if hasattr(e, 'response') else 'Unknown'}"
                logger.warning(f"請求失敗，5秒後重試... (錯誤: {error_msg})")
                last_exception = e
                time.sleep(5)  # 等待5秒後重試
                retries += 1
                if retries > max_retries:
                    logger.error(f"所有重試次數已用盡，無法獲取頁面: {url}")
                    if hasattr(e, 'response') and e.response is not None:
                        logger.error(f"HTTP 狀態碼: {e.response.status_code}")
                        logger.error(f"回應標頭: {dict(e.response.headers) if hasattr(e.response, 'headers') else '無'}")
                    return None
                    
            except Exception as e:
                logger.error(f"處理 {url} 時發生未知錯誤: {str(e)}", exc_info=True)
                last_exception = e
                time.sleep(5)  # 等待5秒後重試
                retries += 1
                if retries > max_retries:
                    logger.error(f"所有重試次數已用盡，無法獲取頁面: {url}")
                    return None
        
        return None
        
    def get_region_list(self) -> List[Dict[str, str]]:
        """獲取所有區域列表，但只返回台北區域"""
        # Based on user feedback, Taipei region code is 'a02'
        # URL format: https://www.atmovies.com.tw/showtime/a02/
        region_info_list = [{'code': 'a02', 'name': '台北市'}] 
        
        processed_regions = []
        for r_info in region_info_list:
            # BASE_URL is "https://www.atmovies.com.tw/showtime/"
            full_region_url = f"{BASE_URL}{r_info['code']}/"

            processed_regions.append({
                'region_code': r_info['code'],
                'region_name': r_info['name'],
                'url': full_region_url 
            })
            
        if processed_regions:
            logger.info(f"目標區域設定為: {processed_regions[0]['region_name']} (Code: {processed_regions[0]['region_code']}, URL: {processed_regions[0]['url']})")
        else:
            logger.warning("未能設定目標區域。")
        return processed_regions

    def get_theaters_in_region(self, region_code: str, region_name: str) -> List[Dict[str, Any]]:
        """獲取指定區域內的所有電影院"""
        theaters = []
        url = f"{BASE_URL}{region_code}/"
        region_name_to_log = region_name
        
        logger.info(f"正在獲取 {region_name_to_log} 的電影院列表從: {url}")
        
        soup = self.fetch_page(url)
        if not soup:
            logger.error(f"無法獲取區域 {region_name_to_log} 的電影院列表頁面: {url}")
            return theaters
        
        try:
            # 尋找所有電影院連結
            for a in soup.find_all('a'):
                href = a.get('href', '')
                # 電影院連結格式為: /showtime/t{atmovies_theater_id}/{region_code}/
                if '/showtime/t' in href and region_code in href:
                    # 從 URL 解析出 atmovies_theater_id
                    url_parts = href.strip('/').split('/')
                    theater_id = url_parts[2]
                    theater_name = a.text.strip()
                    
                    theaters.append({
                        'id': theater_id,
                        'name': theater_name,
                        'url': f"{BASE_URL}{theater_id}/{region_code}/"
                    })
                    
        except Exception as e:
            logger.error(f"解析區域 {region_name_to_log} 的電影院列表時出錯: {str(e)}", exc_info=True)
        
        return theaters

    def get_movie_schedule(self, movie_url: str) -> List[Dict[str, Any]]:
        """
        獲取電影的放映時間表
        
        Args:
            movie_url: 電影詳情頁面URL
            
        Returns:
            包含放映時間的列表
        """
        logger.info(f"獲取電影放映時間表: {movie_url}")
        soup = self.fetch_page(movie_url)
        if not soup:
            logger.error(f"無法獲取電影頁面: {movie_url}")
            return []
            
        # 解析放映時間表
        schedules = []
        try:
            # 這裡根據實際頁面結構實現解析邏輯
            # 示例：假設時間表在 class 為 'schedule' 的 div 中
            schedule_divs = soup.find_all('div', class_='schedule')
            for div in schedule_divs:
                # 解析時間表內容
                # ... 具體解析邏輯 ...
                pass
                
        except Exception as e:
            logger.error(f"解析放映時間表時出錯: {str(e)}", exc_info=True)
            
        return schedules

    def get_theater_schedule(self, theater: Dict[str, Any], date_range: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """獲取特定電影院在特定日期的所有場次"""
        try:
            # 使用與 atmovies_scraper_v3.py 相同的 URL 格式
            # 注意：theater['id'] 已經包含了 't' 前綴，例如 't02a06'
            # 構建 URL 格式: {BASE_URL}{theater_id}/{region_code}/[date/]
            
            # 從 get_region_list 獲取的 region 中提取 region_code
            region_code = theater.get('region_code', 'a02')  # 默認使用 'a02' 作為台北市的區域代碼
            
            # 構建基本 URL
            showtimes_url = f"{BASE_URL}{theater['id']}/{region_code}/"
            
            # 獲取今天的日期格式化字符串
            today_formatted = datetime.date.today().strftime('%Y/%m/%d')
            
            # 如果不是今天，加上日期參數
            if date_range[0]['formatted'] != today_formatted:
                showtimes_url += f"{date_range[0]['url_param']}/"
            
            logger.info(f"獲取場次資料 URL: {showtimes_url}")
            
            # 增加額外的延遲，避免請求過於頻繁
            time.sleep(1 + random.random() * 2)
            
            soup = self.fetch_page(showtimes_url)
            if not soup:
                logger.warning(f"無法獲取場次資料: {theater['name']} - {date_range[0]['formatted']} 從 {showtimes_url}")
                return None

            # 檢查是否有驗證頁面或錯誤頁面
            if "Just a moment" in str(soup) or "Checking your browser" in str(soup):
                raise Exception("觸發了網站的反爬蟲機制")

            movies_on_date = []
            
            # 嘗試多種可能的選擇器來找到電影列表
            movie_elements = []
            selectors_to_try = [
                'ul.theaterList > li',          # 新的選擇器
                'ul#theaterShowtimeTable li',     # 舊的選擇器
                'div.theaterContent',             # 另一種可能的結構
                'div.film_title',                 # 電影標題
                'div.filmTitle',                  # 另一種電影標題
                'div.movie_title',                # 另一種電影標題
                'h2.filmTitle',                   # 標題標籤
                'div.movie-list-item',            # 電影列表項目
                'div.showtime'                    # 場次資訊
            ]
            
            for selector in selectors_to_try:
                elements = soup.select(selector)
                if elements:
                    movie_elements = elements
                    logger.info(f"使用選擇器 '{selector}' 找到 {len(elements)} 個元素")
                    break
            
            # 如果沒有找到任何元素，嘗試更通用的選擇器
            if not movie_elements:
                logger.warning("使用特定選擇器未找到電影元素，嘗試更通用的選擇器...")
                all_elements = soup.find_all(True, recursive=True)
                movie_elements = [el for el in all_elements if 'movie' in str(el).lower() or 'film' in str(el).lower()]
                if movie_elements:
                    logger.info(f"使用通用選擇器找到 {len(movie_elements)} 個可能相關的元素")
            
            # 解析電影名稱
            for movie_el in movie_elements:
                try:
                    movie_name = None
                    
                    # 嘗試多種方法提取電影名稱
                    name_selectors = [
                        ('filmTitle', 'a'),
                        ('film_title', 'a'),
                        ('movie_title', 'a'),
                        ('filmTitle', None),
                        ('film_title', None),
                        ('movie_title', None),
                        ('title', 'a'),
                        ('name', 'a'),
                        (None, 'h2'),
                        (None, 'h3'),
                        (None, 'h4'),
                        (None, 'a')
                    ]
                    
                    for class_name, tag in name_selectors:
                        if class_name and tag:
                            element = movie_el.find(tag, class_=class_name)
                        elif class_name:
                            element = movie_el.find(class_=class_name)
                        elif tag:
                            element = movie_el.find(tag)
                        
                        if element and element.text.strip():
                            movie_name = element.text.strip()
                            break
                    
                    # 如果找到電影名稱，清理並加入列表
                    if movie_name:
                        # 清理名稱：移除多餘空格、換行符、星號等
                        movie_name = re.sub(r'[\n\r\t\*]+', ' ', movie_name).strip()
                        movie_name = re.sub(r'\s+', ' ', movie_name)
                        movies_on_date.append({"movie_name": movie_name, "time": "N/A"})
                except Exception as e:
                    logger.warning(f"解析電影元素時出錯: {str(e)}")
            
            logger.info(f"在 {theater['name']} 於 {date_range[0]['formatted']} 找到 {len(movies_on_date)} 部電影的場次資訊")
            
            # 如果沒有找到任何電影，記錄頁面內容以便調試
            if not movies_on_date:
                logger.warning(f"未找到任何電影場次，頁面標題: {soup.title.string if soup.title else '無標題'}")
                # 將頁面內容保存到文件以便調試
                debug_dir = os.path.join(OUTPUT_DIR, 'debug')
                os.makedirs(debug_dir, exist_ok=True)
                debug_file = os.path.join(debug_dir, f"debug_{theater['id']}_{date_range[0]['url_param']}.html")
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(str(page_content))
                logger.info(f"已保存調試頁面到: {debug_file}")
            
            return {
                'date': date_range[0]['formatted'],
                'showtimes': movies_on_date
            }
            
        except Exception as e:
            logger.error(f"在處理 {theater.get('name', '未知電影院')} 的場次時發生錯誤: {str(e)}", exc_info=True)
            return None

    def process_theater(self, theater: Dict[str, Any], date_range: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """
        同步處理單個電影院的所有日期場次
        
        Args:
            theater: 電影院資訊字典
        showtimes_by_date = []
        
        # 獲取該電影院在不同日期的場次
        for date_info in date_range:
            date_str = date_info['formatted']
            date_param = date_info['url_param']
            
            logger.info(f"處理 {theater_name} 在 {date_str} 的場次")
            
            # 獲取該日期的場次
            showtimes = self.get_theater_schedule(theater, [date_info])
                continue
                
            logger.info(f"在區域 {region.get('name', '未知區域')} 找到 {len(theaters)} 家電影院")
{{ ... }}
            # 如果指定了最大電影院數量，則進行限制
            if max_theaters_to_scrape is not None:
                theaters = theaters[:max_theaters_to_scrape]
                
            # 處理每家電影院
            for theater in theaters:
                try:
                    self.process_theater(theater, date_range)
                except Exception as e:
                    logger.error(f"處理電影院 {theater.get('name', '未知電影院')} 時出錯: {str(e)}", exc_info=True)
                    continue
                    
    except Exception as e:
        logger.error(f"爬蟲執行過程中發生錯誤: {str(e)}", exc_info=True)
        raise
        
        # 初始化統計變數
        total_movies_count = 0
        total_dates_with_data = 0
        all_theaters_stats = []
        
        # 處理每家電影院的場次資料
        for theater in self.data:
            theater_stats = {
                'theater_id': theater.get('theater_id', 'unknown'),
                'theater_name': theater.get('name', '未知電影院'),
                'dates': []
            }
            
            showtimes_dates = []
            theater_data = theater.get('data', {})
            
            for date_data in theater_data.get('atmovies_showtimes_by_date', []):
                date_formatted = date_data.get('date', '未知日期')
                showtimes = date_data.get('showtimes', [])
                movie_count = len(showtimes)
                
                if movie_count > 0:
                    showtimes_dates.append(date_formatted)
                    total_movies_count += movie_count
                    total_dates_with_data += 1
                
                theater_stats['dates'].append({
                    'date': date_formatted,
                    'movie_count': movie_count,
                    'movies': [movie.get('movie_name', '未知電影') for movie in showtimes]
                })
            
            all_theaters_stats.append(theater_stats)
            
            print("\n=== ATMOVIES DATA UPDATE CHECK ===")
            print(f"Timestamp (Taipei Time): {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"Theater Scraped: {theater_name} (ID: {theater_id})")
            if showtimes_dates:
                print(f"Showtime Dates Found: {', '.join(showtimes_dates)}")
            else:
                print(f"Showtime Dates Found: None for this theater on queried dates ({dates_to_check}).")
            print("==============================\n")
        
        # 在所有電影院處理完成後輸出統計信息
        print("\n=== ATMOVIES DATA UPDATE STATISTICS ===")
        print(f"Timestamp (Taipei Time): {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Total Theaters Scraped: {len(self.data)}")
        print(f"Total Dates with Data: {total_dates_with_data}")
        print(f"Total Movies Found: {total_movies_count}")
        print("\nDetailed Statistics by Theater and Date:")
        
        for theater in all_theaters_stats:
            print(f"\n{theater['theater_name']} (ID: {theater['theater_id']})")
            for date_info in theater['dates']:
                print(f"  {date_info['date']}: {date_info['movie_count']} movies")
                if date_info['movie_count'] > 0:
                    for i, movie in enumerate(date_info['movies'], 1):
                        print(f"    {i}. {movie}")
        
        print("\n==============================")
        
        # 返回統計信息供其他函數使用
        return {
            'total_theaters': len(self.data),
            'total_dates_with_data': total_dates_with_data,
            'total_movies': total_movies_count,
            'theater_stats': all_theaters_stats
        }
    
    # self.data is populated but not explicitly saved in this script's main flow

def main():
    start_time = time.time()
    logger.info(f"開始執行 ATMovies 資料更新檢查爬蟲，時間: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    max_theaters = int(os.environ.get("MAX_THEATERS_TO_CHECK", MAX_THEATERS_TO_CHECK_DEFAULT))
    logger.info(f"將檢查最多 {max_theaters} 家電影院的資料。")
    logger.info(f"設置為抓取 3 天的場次資料 (今天、明天、後天)，最大並發請求數: {MAX_CONCURRENT_REQUESTS}")
    
    scraper = ATMoviesScraper()
    scraper.scrape_all(max_theaters_to_scrape=max_theaters)
    
    end_time = time.time()
    duration = end_time - start_time
    logger.info(f"資料更新檢查爬蟲總耗時: {duration:.2f} 秒 ({duration/60:.2f} 分鐘)")
    logger.info(f"完成執行資料更新檢查爬蟲，時間: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    # 主要輸出將來自 process_theater 中的 print 語句

if __name__ == "__main__":
    # In Python 3.7+ asyncio.run is preferred
    # For older versions, you might use loop = asyncio.get_event_loop(); loop.run_until_complete(main())
    main()

